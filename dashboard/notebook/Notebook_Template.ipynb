{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1 content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# house_price_prediction.ipynb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Import essential libraries\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# house_price_prediction.ipynb\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNetCV, LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Ignore warnings for clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the directory where the script is located\n",
    "BASE_DIR = Path(_file_).resolve().parent\n",
    "\n",
    "# Define directories\n",
    "data_dir = BASE_DIR / 'dashboard' / 'notebook' / 'data'\n",
    "models_dir = BASE_DIR / 'models'  # Subdirectory for models\n",
    "os.makedirs(models_dir, exist_ok=True)  # Ensure models directory exists\n",
    "\n",
    "# Set file paths\n",
    "house_data_file = BASE_DIR / 'dashboard' / 'data' / 'house_prices_records.csv'\n",
    "inherited_houses_file = BASE_DIR / 'dashboard' / 'data' / 'inherited_houses.csv'\n",
    "\n",
    "# Import datasets\n",
    "house_data = pd.read_csv(house_data_file)\n",
    "inherited_houses = pd.read_csv(inherited_houses_file)\n",
    "\n",
    "print(f\"House Data Shape: {house_data.shape}\")\n",
    "print(f\"Inherited Houses Shape: {inherited_houses.shape}\")\n",
    "\n",
    "# Display first few rows of the datasets\n",
    "print(\"First few rows of house_data:\")\n",
    "print(house_data.head())\n",
    "print(\"First few rows of inherited_houses:\")\n",
    "print(inherited_houses.head())\n",
    "\n",
    "# Apply log transformation to SalePrice\n",
    "# The sale prices are right-skewed; applying log transformation to normalize the distribution\n",
    "house_data['SalePrice_Log'] = np.log1p(house_data['SalePrice'])\n",
    "\n",
    "# Handle missing values in house_data\n",
    "print(\"\\nHandling missing values in house_data...\")\n",
    "\n",
    "# List of features where missing values likely indicate absence of the feature\n",
    "zero_fill_features = ['2ndFlrSF', 'EnclosedPorch', 'MasVnrArea', 'WoodDeckSF',\n",
    "                      'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', 'BsmtUnfSF']\n",
    "\n",
    "for feature in zero_fill_features:\n",
    "    house_data[feature].fillna(0, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with 0.\")\n",
    "\n",
    "# Fill missing categorical features with mode or default value\n",
    "categorical_mode_fill = {\n",
    "    'BedroomAbvGr': house_data['BedroomAbvGr'].mode()[0],\n",
    "    'BsmtFinType1': 'None',\n",
    "    'GarageFinish': 'Unf',\n",
    "    'BsmtExposure': 'No',\n",
    "    'KitchenQual': 'TA'\n",
    "}\n",
    "\n",
    "for feature, value in categorical_mode_fill.items():\n",
    "    house_data[feature].fillna(value, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with '{value}'.\")\n",
    "\n",
    "# Fill missing numerical features with median\n",
    "numerical_median_fill = ['GarageYrBlt', 'LotFrontage', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd']\n",
    "\n",
    "for feature in numerical_median_fill:\n",
    "    median_value = house_data[feature].median()\n",
    "    house_data[feature].fillna(median_value, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with median value {median_value}.\")\n",
    "\n",
    "# Verify that there are no missing values left\n",
    "print(\"\\nChecking for remaining missing values:\")\n",
    "print(house_data.isnull().sum()[house_data.isnull().sum() > 0])\n",
    "\n",
    "# Encode categorical features\n",
    "print(\"\\nEncoding categorical features in house_data...\")\n",
    "\n",
    "# Define mappings for ordinal categorical features based on their definitions\n",
    "ordinal_mappings = {\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "}\n",
    "\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    if col in house_data.columns:\n",
    "        house_data[col] = house_data[col].map(mapping)\n",
    "        print(f\"Encoded {col} using ordinal mapping.\")\n",
    "\n",
    "# Identify numeric features\n",
    "numeric_feats = house_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Check skewness of numeric features\n",
    "skewness = house_data[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "print(\"\\nSkewness of numeric features:\")\n",
    "print(skewness)\n",
    "\n",
    "# Features with high skewness (threshold can be adjusted)\n",
    "skewed_features = skewness[abs(skewness) > 0.75].index.tolist()\n",
    "print(\"\\nFeatures with high skewness (|skewness| > 0.75):\")\n",
    "print(skewed_features)\n",
    "\n",
    "# Apply log or box-cox transformation to skewed features\n",
    "print(\"\\nTransforming skewed features in house_data...\")\n",
    "\n",
    "# Dictionary to store lambda values for box-cox transformation\n",
    "lam_dict = {}\n",
    "\n",
    "for feat in skewed_features:\n",
    "    if (house_data[feat] <= 0).any():\n",
    "        # If the feature has zero or negative values, use log1p transformation\n",
    "        house_data[feat] = np.log1p(house_data[feat])\n",
    "        print(f\"Applied log1p transformation to {feat}.\")\n",
    "    else:\n",
    "        # Apply box-cox transformation\n",
    "        try:\n",
    "            transformed_data, lam = boxcox(house_data[feat])\n",
    "            house_data[feat] = transformed_data\n",
    "            lam_dict[feat] = lam\n",
    "            print(f\"Applied box-cox transformation to {feat} with lambda {lam:.4f}.\")\n",
    "        except ValueError:\n",
    "            # If box-cox fails, use log1p\n",
    "            house_data[feat] = np.log1p(house_data[feat])\n",
    "            print(f\"Applied log1p transformation to {feat} (box-cox failed).\")\n",
    "\n",
    "# Save skewed features and lambda values for future use\n",
    "with open(models_dir / 'skewed_features.pkl', 'wb') as f:\n",
    "    pickle.dump(skewed_features, f)\n",
    "with open(models_dir / 'lam_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(lam_dict, f)\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nPerforming feature engineering in house_data...\")\n",
    "\n",
    "# Create new features based on domain knowledge\n",
    "house_data['TotalSF'] = house_data['TotalBsmtSF'] + house_data['1stFlrSF'] + house_data['2ndFlrSF']\n",
    "print(\"Created TotalSF feature as sum of TotalBsmtSF, 1stFlrSF, and 2ndFlrSF.\")\n",
    "\n",
    "house_data['Qual_TotalSF'] = house_data['OverallQual'] * house_data['TotalSF']\n",
    "print(\"Created Qual_TotalSF feature as product of OverallQual and TotalSF.\")\n",
    "\n",
    "# Prepare data for modeling\n",
    "print(\"\\nPreparing data for modeling...\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "X = house_data.drop(['SalePrice', 'SalePrice_Log'], axis=1, errors='ignore')\n",
    "y = house_data['SalePrice_Log']\n",
    "\n",
    "# Define the features based on the provided metadata\n",
    "feature_list = [\n",
    "    '1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtFinType1',\n",
    "    'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea', 'GarageFinish',\n",
    "    'GarageYrBlt', 'GrLivArea', 'KitchenQual', 'LotArea', 'LotFrontage',\n",
    "    'MasVnrArea', 'EnclosedPorch', 'OpenPorchSF', 'OverallCond', 'OverallQual',\n",
    "    'WoodDeckSF', 'YearBuilt', 'YearRemodAdd', 'TotalSF', 'Qual_TotalSF'  # Include engineered features\n",
    "]\n",
    "\n",
    "# Ensure the features are in X\n",
    "X = X[feature_list]\n",
    "\n",
    "# Feature selection using Random Forest\n",
    "print(\"\\nPerforming feature selection using Random Forest...\")\n",
    "\n",
    "# Use Random Forest to estimate feature importances\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Feature importances from Random Forest:\")\n",
    "print(importances)\n",
    "\n",
    "# Select top features (e.g., top 20)\n",
    "selected_features = importances[:20].index.tolist()\n",
    "print(\"\\nSelected top features for modeling:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Save selected features for future use\n",
    "with open(models_dir / 'selected_features.pkl', 'wb') as f:\n",
    "    pickle.dump(selected_features, f)\n",
    "\n",
    "# Keep only selected features\n",
    "X = X[selected_features]\n",
    "\n",
    "# Split data into training and test sets\n",
    "print(\"\\nSplitting data into training and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save train and test data for the dashboard\n",
    "joblib.dump((X_train, X_test, y_train, y_test), models_dir / 'train_test_data.joblib')\n",
    "\n",
    "# Scaling features\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for future use\n",
    "joblib.dump(scaler, models_dir / 'scaler.joblib')\n",
    "\n",
    "# Model training\n",
    "print(\"\\nTraining models...\")\n",
    "\n",
    "# Adjusted alpha values for Ridge Regression and Lasso Regression to avoid numerical instability\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': RidgeCV(alphas=np.logspace(-3, 3, 7), cv=5),\n",
    "    'ElasticNet': ElasticNetCV(alphas=np.logspace(-4, -0.5, 30), l1_ratio=[0.1, 0.5, 0.9], cv=5, max_iter=10000),\n",
    "    'Lasso Regression': LassoCV(alphas=np.logspace(-3, -0.5, 30), cv=5, max_iter=10000),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3,\n",
    "        min_samples_leaf=5, max_features=0.8, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=None, max_features='sqrt',\n",
    "        min_samples_leaf=2, random_state=42),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=5,\n",
    "        min_child_weight=3, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "}\n",
    "\n",
    "# Model evaluation\n",
    "print(\"\\nEvaluating models...\")\n",
    "results = {'Model': [], 'MAE': [], 'RMSE': [], 'R² Score': []}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    model_filename = f\"{name.replace(' ', '_').lower()}_model.joblib\"\n",
    "    # Save the trained model\n",
    "    joblib.dump(model, os.path.join(models_dir, model_filename))\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    # Calculate performance metrics\n",
    "    y_test_exp = np.expm1(y_test)\n",
    "    predictions_exp = np.expm1(predictions)\n",
    "    # Handle any negative predictions due to model limitations\n",
    "    predictions_exp[predictions_exp < 0] = 0\n",
    "    mae = mean_absolute_error(y_test_exp, predictions_exp)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_exp, predictions_exp))\n",
    "    r2 = r2_score(y_test_exp, predictions_exp)\n",
    "    # Store results\n",
    "    results['Model'].append(name)\n",
    "    results['MAE'].append(mae)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['R² Score'].append(r2)\n",
    "    print(f\"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R² Score: {r2:.4f}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(os.path.join(models_dir, 'model_evaluation.csv'), index=False)\n",
    "\n",
    "# Save feature importances\n",
    "# Using the 'importances' Series from Random Forest\n",
    "feature_importances = importances.reset_index()\n",
    "feature_importances.columns = ['Feature', 'Importance']\n",
    "feature_importances.to_csv(os.path.join(models_dir, 'feature_importances.csv'), index=False)\n",
    "print(\"\\nSaved feature importances to 'feature_importances.csv'.\")\n",
    "\n",
    "# Process inherited houses\n",
    "print(\"\\nProcessing inherited houses...\")\n",
    "\n",
    "# Handle missing values in inherited_houses\n",
    "print(\"Handling missing values in inherited_houses...\")\n",
    "for feature in zero_fill_features:\n",
    "    inherited_houses[feature].fillna(0, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with 0.\")\n",
    "\n",
    "for feature, value in categorical_mode_fill.items():\n",
    "    inherited_houses[feature].fillna(value, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with '{value}'.\")\n",
    "\n",
    "for feature in numerical_median_fill:\n",
    "    median_value = house_data[feature].median()\n",
    "    inherited_houses[feature].fillna(median_value, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with median value {median_value}.\")\n",
    "\n",
    "# Encode categorical features\n",
    "print(\"Encoding categorical features in inherited_houses...\")\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    if col in inherited_houses.columns:\n",
    "        inherited_houses[col] = inherited_houses[col].map(mapping)\n",
    "        print(f\"Encoded {col} using ordinal mapping.\")\n",
    "\n",
    "# Feature engineering on inherited houses\n",
    "print(\"Performing feature engineering on inherited_houses...\")\n",
    "inherited_houses['TotalSF'] = inherited_houses['TotalBsmtSF'] + inherited_houses['1stFlrSF'] + inherited_houses['2ndFlrSF']\n",
    "print(\"Created TotalSF feature.\")\n",
    "inherited_houses['Qual_TotalSF'] = inherited_houses['OverallQual'] * inherited_houses['TotalSF']\n",
    "print(\"Created Qual_TotalSF feature.\")\n",
    "\n",
    "# Transform skewed features\n",
    "print(\"\\nTransforming skewed features in inherited_houses...\")\n",
    "for feat in skewed_features:\n",
    "    if feat in inherited_houses.columns:\n",
    "        if (inherited_houses[feat] <= 0).any():\n",
    "            inherited_houses[feat] = np.log1p(inherited_houses[feat])\n",
    "            print(f\"Applied log1p transformation to {feat}.\")\n",
    "        else:\n",
    "            lam = lam_dict.get(feat)\n",
    "            if lam is not None:\n",
    "                try:\n",
    "                    inherited_houses[feat] = boxcox(inherited_houses[feat], lam)\n",
    "                    print(f\"Applied box-cox transformation to {feat} with lambda {lam:.4f}.\")\n",
    "                except ValueError:\n",
    "                    inherited_houses[feat] = np.log1p(inherited_houses[feat])\n",
    "                    print(f\"Applied log1p transformation to {feat} (box-cox failed).\")\n",
    "            else:\n",
    "                inherited_houses[feat] = np.log1p(inherited_houses[feat])\n",
    "                print(f\"Applied log1p transformation to {feat} (no lambda found).\")\n",
    "\n",
    "# Ensure the features match\n",
    "inherited_houses = inherited_houses.reindex(columns=selected_features, fill_value=0)\n",
    "print(\"\\nReindexed inherited_houses to match selected features.\")\n",
    "\n",
    "# Scaling\n",
    "print(\"Scaling inherited houses features...\")\n",
    "inherited_houses_scaled = scaler.transform(inherited_houses)\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nMaking predictions on inherited houses...\")\n",
    "predictions_df = pd.DataFrame()\n",
    "for name, model in models.items():\n",
    "    predictions_log = model.predict(inherited_houses_scaled)\n",
    "    predictions_actual = np.expm1(predictions_log)\n",
    "    # Handle negative predictions\n",
    "    predictions_actual[predictions_actual < 0] = 0\n",
    "    # Store predictions\n",
    "    predictions_df[name] = predictions_actual\n",
    "    print(f\"Predictions made using {name}.\")\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv(os.path.join(models_dir, 'inherited_houses_predictions.csv'), index=False)\n",
    "print(\"\\nPredictions saved to 'inherited_houses_predictions.csv'.\")\n",
    "\n",
    "# Optional: Display the predictions\n",
    "print(\"\\nPredictions for Inherited Houses:\")\n",
    "print(predictions_df)\n",
    "\n",
    "# Save the final model (best performing model)\n",
    "best_model_name = results_df.sort_values('RMSE').iloc[0]['Model']\n",
    "print(f\"\\nBest performing model is {best_model_name}. Saving as final_model.joblib.\")\n",
    "joblib.dump(models[best_model_name], os.path.join(models_dir, 'final_model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
